from typing import List, Tuple, Dict, Optional
import os, subprocess, tempfile
import math
from tempfile import NamedTemporaryFile
from pathlib import Path
from shutil import which
import numpy as np
import cv2, logging
import mediapipe as mp
from fastapi import HTTPException
from sqlalchemy.orm import Session

from app.config import settings
from app.models.feedback_summary import FeedbackSummary
from app.models.media_asset import MediaAsset
from app.services.storage_service import supabase, VIDEO_BUCKET as BUCKET_NAME

logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "%(levelname)s:%(name)s:%(message)s"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.DEBUG)
logger.propagate = False

GAZE_OFF_ABS = 0.12
EYE_OFF_ABS = 0.35
BLINK_RATIO = 0.75
BLINK_MIN_DUR = 0.08
EMA_ALPHA = 0.25
MOUTH_DELTA = 0.02

# FaceMesh ì„¸íŒ…
mp_face_mesh = mp.solutions.face_mesh
FACE_MESH = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5,
)

# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤
LEFT_EYE = [33, 160, 158, 133, 153, 144]
RIGHT_EYE = [263, 387, 385, 362, 380, 373]
LEFT_EYE_CORNERS = (33, 133)
RIGHT_EYE_CORNERS = (263, 362)
LEFT_EYE_LIDS = (159, 145)
RIGHT_EYE_LIDS = (386, 374)
LEFT_IRIS_IDXS = [474, 475, 476, 477]
RIGHT_IRIS_IDXS = [469, 470, 471, 472]

MOUTH_LEFT_CORNER = 61
MOUTH_RIGHT_CORNER = 291
MOUTH_UPPER_INNER = 13
MOUTH_LOWER_INNER = 14
NOSE_TIP = 1


def norm_pt(lm, w, h):
    return np.array([lm.x * w, lm.y * h, lm.z], dtype=np.float32)


def head_pose_proxy(lms, w, h):
    nose = norm_pt(lms[NOSE_TIP], w, h)
    cx, cy = w / 2.0, h / 2.0
    return float((nose[0] - cx) / w), float((nose[1] - cy) / h)  # (yaw, pitch ëŒ€ìš©)


def ear_from_landmarks(lms, w, h, idxs):
    pts = [norm_pt(lms[i], w, h) for i in idxs]
    p1, p2, p3, p4, p5, p6 = pts
    dv1 = np.linalg.norm(p2[:2] - p6[:2])
    dv2 = np.linalg.norm(p3[:2] - p5[:2])
    dh = np.linalg.norm(p1[:2] - p4[:2])
    if dh == 0:
        return 0.0
    return float((dv1 + dv2) / (2.0 * dh))


def mouth_corners_relative(lms, w, h):
    left = norm_pt(lms[MOUTH_LEFT_CORNER], w, h)
    right = norm_pt(lms[MOUTH_RIGHT_CORNER], w, h)
    up_in = norm_pt(lms[MOUTH_UPPER_INNER], w, h)
    lo_in = norm_pt(lms[MOUTH_LOWER_INNER], w, h)
    center = (up_in + lo_in) / 2.0
    rel_left = (left[1] - center[1]) / h
    rel_right = (right[1] - center[1]) / h
    return float((rel_left + rel_right) / 2.0)


def iris_centers_from_landmarks(lms, w, h):
    L = [norm_pt(lms[i], w, h) for i in LEFT_IRIS_IDXS if i < len(lms)]
    R = [norm_pt(lms[i], w, h) for i in RIGHT_IRIS_IDXS if i < len(lms)]
    lc = np.mean(np.array(L), axis=0) if len(L) >= 3 else None
    rc = np.mean(np.array(R), axis=0) if len(R) >= 3 else None
    return lc, rc


def eye_local_axes(lms, w, h, corners, lids):
    c_out = norm_pt(lms[corners[0]], w, h)
    c_in = norm_pt(lms[corners[1]], w, h)
    e_center = (c_out + c_in) / 2.0
    ex = c_in[:2] - c_out[:2]
    exn = ex / (np.linalg.norm(ex) + 1e-6)
    lid_up = norm_pt(lms[lids[0]], w, h)
    lid_dn = norm_pt(lms[lids[1]], w, h)
    ey = lid_dn[:2] - lid_up[:2]
    eyn = ey / (np.linalg.norm(ey) + 1e-6)
    w_eye = np.linalg.norm(ex)
    h_eye = np.linalg.norm(ey)
    return e_center, exn, eyn, w_eye, h_eye


def eye_gaze_offset(lms, w, h, iris_center, corners, lids):
    if iris_center is None:
        return None
    e_center, ex, ey, w_eye, h_eye = eye_local_axes(lms, w, h, corners, lids)
    d = iris_center[:2] - e_center[:2]
    hor = float(np.dot(d, ex) / (w_eye + 1e-6))
    ver = float(np.dot(d, ey) / (h_eye + 1e-6))
    return hor, ver


def ema_update(prev, new, alpha=EMA_ALPHA):
    if prev is None:
        return new
    return (1 - alpha) * prev + alpha * new


def compute_fixation_metrics(xs: List[float], ys: List[float], target=(0.0, 0.0)):
    if not xs or not ys or len(xs) != len(ys):
        return {"MAE": None, "SDx": None, "SDy": None, "rho": None, "BCEA": None, "S2S": None}
    X = np.array(xs, dtype=np.float32)
    Y = np.array(ys, dtype=np.float32)
    tx, ty = target
    MAE = float(np.mean(np.sqrt((X - tx) ** 2 + (Y - ty) ** 2)))
    SDx = float(np.std(X, ddof=0))
    SDy = float(np.std(Y, ddof=0))
    if len(X) > 1:
        rho = float(np.corrcoef(X, Y)[0, 1])
        if np.isnan(rho):
            rho = 0.0
    else:
        rho = 0.0
    k = 1.14
    base = max(0.0, 1.0 - rho**2)
    BCEA = float(2 * k * SDx * SDy * math.sqrt(base))
    if len(X) > 1:
        diffs = np.sqrt(np.diff(X) ** 2 + np.diff(Y) ** 2)
        S2S = float(np.sqrt(np.mean(diffs**2)))
    else:
        S2S = None
    return {"MAE": MAE, "SDx": SDx, "SDy": SDy, "rho": rho, "BCEA": BCEA, "S2S": S2S}


def safe_num(x: Optional[float]) -> Optional[float]:
    try:
        v = float(x)
        return v if math.isfinite(v) else None
    except Exception:
        return None


def grade_from_rate(rate: float) -> str:
    # 0~1 ë²”ìœ„ ê°€ì •
    if rate >= 0.8:
        return "ì–‘í˜¸"
    if rate >= 0.6:
        return "ë³´í†µ"
    return "ê°œì„ í•„ìš”"


def grade_mouth(delta: float) -> str:
    if delta <= -MOUTH_DELTA:
        return "ë¯¸ì†Œ"
    if delta >= MOUTH_DELTA:
        return "í•˜ê°•"
    return "ì¤‘ë¦½"


def build_feedback_summary(gaze_grade: str, blink_grade: str, mouth_grade: str) -> str:
    parts = []
    # ì£¼ì‹œ
    if gaze_grade == "ì–‘í˜¸":
        parts.append("ì •ë©´ ì£¼ì‹œìœ¨ì€ ì–‘í˜¸í•©ë‹ˆë‹¤")
    elif gaze_grade == "ë³´í†µ":
        parts.append("ì •ë©´ ì£¼ì‹œìœ¨ì€ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤")
    else:
        parts.append("ì •ë©´ ì£¼ì‹œìœ¨ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")

    # ê¹œë¹¡ì„
    if blink_grade == "ì–‘í˜¸":
        parts.append("ê¹œë¹¡ì„ ì•ˆì •ë„ëŠ” ì–‘í˜¸í•©ë‹ˆë‹¤")
    elif blink_grade == "ë³´í†µ":
        parts.append("ê¹œë¹¡ì„ ì•ˆì •ë„ëŠ” ë³´í†µì…ë‹ˆë‹¤")
    else:
        parts.append("ê¹œë¹¡ì„ ë¹ˆë„ë¥¼ ì•ˆì •í™”í•˜ì„¸ìš”")

    # ì…ê¼¬ë¦¬
    if mouth_grade == "ë¯¸ì†Œ":
        parts.append("ì…ê¼¬ë¦¬ëŠ” ìƒìŠ¹ ê²½í–¥(ë¯¸ì†Œ)ì…ë‹ˆë‹¤")
    elif mouth_grade == "í•˜ê°•":
        parts.append("ì…ê¼¬ë¦¬ í•˜ê°• ê²½í–¥ì´ ê´€ì°°ë©ë‹ˆë‹¤")
    else:
        parts.append("ì…ê¼¬ë¦¬ëŠ” ëŒ€ì²´ë¡œ ì¤‘ë¦½ì…ë‹ˆë‹¤")

    return " / ".join(parts) + "."


def analyze_expression_video(
    video_path: str,
    blink_limit_per_min: int = 30,
    baseline_seconds: float = 2.0,
    frame_stride: int = 5,
) -> Dict:
    logger.info(
        "[EXPR] START video_path=%s blink_limit_per_min=%s baseline_seconds=%.2f frame_stride=%s",
        video_path,
        blink_limit_per_min,
        baseline_seconds,
        frame_stride,
    )

    cap = cv2.VideoCapture(video_path)
    opened = cap.isOpened()
    logger.info("[EXPR] VideoCapture opened=%s path=%s", opened, video_path)

    if not opened:
        logger.error("[EXPR] Failed to open video: %s", video_path)
        raise FileNotFoundError("Session video not found")

    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    if fps <= 0 or np.isnan(fps):
        logger.warning("[EXPR] Invalid FPS detected: %s â†’ fallback to 30.0", fps)
        fps = 30.0

    logger.info("[EXPR] FPS=%s", fps)

    vbuf: List[Tuple[float, float, float, float, float, float]] = []
    baseline: Optional[Dict[str, float]] = None
    baseline_from_video = False

    stats = {
        "frames": 0,
        "frames_head_ok": 0,
        "frames_eye_valid": 0,
        "frames_eye_ok": 0,
        "frames_head_eye_ok": 0,
        "blinks_count": 0,
    }
    ema = {"yaw": None, "pitch": None, "ear": None, "mouth": None, "eye_h": None, "eye_v": None}
    blink_in_progress = False
    last_blink_t = 0.0

    gaze_head_x: List[float] = []
    gaze_head_y: List[float] = []
    gaze_both_x: List[float] = []
    gaze_both_y: List[float] = []

    idx = -1
    logged_progress_step = 100  # ëª‡ í”„ë ˆì„ë§ˆë‹¤ ì§„í–‰ ë¡œê·¸ í•œ ë²ˆì”© ì°ì„ì§€
    raw_frames_total = 0
    frames_with_face = 0
    logged_progress_step = 100  # ëª‡ í”„ë ˆì„ë§ˆë‹¤ ë¡œê·¸ ì°ì„ì§€

    while True:
        ok, frame0 = cap.read()
        if not ok:
            logger.debug("[EXPR] cap.read() returned False â†’ loop break")
            break

        raw_frames_total += 1

        idx += 1
        # stride ì ìš©
        if frame_stride > 1 and (idx % frame_stride) != 0:
            continue
        t = idx / fps

        h0, w0 = frame0.shape[:2]
        if w0 > 640:
            scale = 640.0 / w0
            frame = cv2.resize(frame0, (640, int(h0 * scale)), interpolation=cv2.INTER_AREA)
        else:
            frame = frame0

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        res = FACE_MESH.process(rgb)
        if not res.multi_face_landmarks:
            if raw_frames_total % logged_progress_step == 0:
                logger.debug(
                    "[EXPR] no_face_detected_at_frame raw_idx=%s t=%.2fs",
                    raw_frames_total,
                    t,
                )
            continue
        lms = res.multi_face_landmarks[0].landmark
        h, w = frame.shape[:2]

        yaw, pitch = head_pose_proxy(lms, w, h)
        ear_l = ear_from_landmarks(lms, w, h, LEFT_EYE)
        ear_r = ear_from_landmarks(lms, w, h, RIGHT_EYE)
        ear = (ear_l + ear_r) / 2.0
        mouth = mouth_corners_relative(lms, w, h)

        lc, rc = iris_centers_from_landmarks(lms, w, h)
        l_off = eye_gaze_offset(lms, w, h, lc, LEFT_EYE_CORNERS, LEFT_EYE_LIDS) if lc is not None else None
        r_off = eye_gaze_offset(lms, w, h, rc, RIGHT_EYE_CORNERS, RIGHT_EYE_LIDS) if rc is not None else None
        eye_h = np.mean([x[0] for x in [l_off, r_off] if x is not None]) if (l_off or r_off) else None
        eye_v = np.mean([x[1] for x in [l_off, r_off] if x is not None]) if (l_off or r_off) else None

        stats["frames"] += 1
        frames_with_face += 1

        # progress ë¡œê·¸ (ë„ˆë¬´ ë§ì´ ì•ˆ ì°íˆê²Œ í”„ë ˆì„ ê°„ê²© ì¡°ì ˆ)
        if stats["frames"] % logged_progress_step == 0:
            logger.info(
                "[EXPR] progress frames=%s t=%.2fs baseline_set=%s blinks=%s",
                stats["frames"],
                t,
                baseline is not None,
                stats["blinks_count"],
            )

        # baseline ìˆ˜ì§‘
        if baseline is None and t <= baseline_seconds:
            vbuf.append((yaw, pitch, ear, mouth, eye_h or 0.0, eye_v or 0.0))

        # EMA ì—…ë°ì´íŠ¸
        ema["yaw"] = ema_update(ema["yaw"], yaw)
        ema["pitch"] = ema_update(ema["pitch"], pitch)
        ema["ear"] = ema_update(ema["ear"], ear)
        ema["mouth"] = ema_update(ema["mouth"], mouth)
        if eye_h is not None:
            ema["eye_h"] = ema_update(ema["eye_h"], eye_h)
        if eye_v is not None:
            ema["eye_v"] = ema_update(ema["eye_v"], eye_v)

        # baseline ê²°ì •
        if baseline is None and t > baseline_seconds:
            if vbuf:
                arr = np.array(vbuf, dtype=np.float32)
                yaw_b, pitch_b, ear_b, mouth_b, eye_h_b, eye_v_b = np.median(arr, axis=0)
                baseline = {
                    "yaw": float(yaw_b),
                    "pitch": float(pitch_b),
                    "ear": float(max(1e-6, ear_b)),
                    "mouth": float(mouth_b),
                    "eye_h": float(eye_h_b),
                    "eye_v": float(eye_v_b),
                }
                logger.info("[EXPR] baseline computed from buffer (median of %d frames)", len(vbuf))
            else:
                baseline = {
                    "yaw": yaw,
                    "pitch": pitch,
                    "ear": max(1e-6, ear),
                    "mouth": mouth,
                    "eye_h": eye_h or 0.0,
                    "eye_v": eye_v or 0.0,
                }
                logger.info("[EXPR] baseline initialized from current frame")
            baseline_from_video = True

        if baseline is None:
            baseline = {
                "yaw": yaw,
                "pitch": pitch,
                "ear": max(1e-6, ear),
                "mouth": mouth,
                "eye_h": eye_h or 0.0,
                "eye_v": eye_v or 0.0,
            }
            baseline_from_video = True
            logger.info("[EXPR] baseline forced initialization (early frame)")

        # íŒì •(ë¨¸ë¦¬)
        dyaw = (ema["yaw"] - baseline["yaw"]) if ema["yaw"] is not None else 0.0
        dpitch = (ema["pitch"] - baseline["pitch"]) if ema["pitch"] is not None else 0.0
        HEAD_OK = (abs(dyaw) <= GAZE_OFF_ABS) and (abs(dpitch) <= GAZE_OFF_ABS)
        if HEAD_OK:
            stats["frames_head_ok"] += 1

        # íŒì •(ëˆˆ)
        EYE_OK = False
        eye_h_corr, eye_v_corr = None, None
        if (ema["eye_h"] is not None) and (ema["eye_v"] is not None):
            stats["frames_eye_valid"] += 1
            eye_h_corr = ema["eye_h"] - baseline.get("eye_h", 0.0)
            eye_v_corr = ema["eye_v"] - baseline.get("eye_v", 0.0)
            EYE_OK = (abs(eye_h_corr) <= EYE_OFF_ABS) and (abs(eye_v_corr) <= EYE_OFF_ABS)
            if EYE_OK:
                stats["frames_eye_ok"] += 1

        # ê²°í•©
        HEAD_EYE_OK = HEAD_OK and EYE_OK
        if HEAD_EYE_OK:
            stats["frames_head_eye_ok"] += 1

        # ê¹œë¹¡ì„ ì¹´ìš´íŠ¸(EAR ê¸°ë°˜)
        eye_closed = (ema["ear"] is not None) and (ema["ear"] < baseline["ear"] * BLINK_RATIO)
        tsec = idx / fps
        if eye_closed and not blink_in_progress:
            blink_in_progress = True
            last_blink_t = tsec
        elif (not eye_closed) and blink_in_progress:
            if (tsec - last_blink_t) >= BLINK_MIN_DUR:
                stats["blinks_count"] += 1
            blink_in_progress = False

        # ì¢Œí‘œ ì €ì¥(ì‹œì„  ì§€í‘œ)
        gaze_head_x.append(float(dyaw))
        gaze_head_y.append(float(dpitch))
        if HEAD_EYE_OK and (eye_h_corr is not None) and (eye_v_corr is not None):
            gaze_both_x.append(float(eye_h_corr))
            gaze_both_y.append(float(eye_v_corr))

    frames_total = stats["frames"]
    cap.release()

    logger.info(
        "[EXPR] LOOP_END raw_frames_total=%s frames_with_face=%s "
        "frames_head_ok=%s frames_eye_valid=%s frames_eye_ok=%s "
        "frames_head_eye_ok=%s blinks=%s",
        raw_frames_total,
        frames_with_face,
        stats["frames_head_ok"],
        stats["frames_eye_valid"],
        stats["frames_eye_ok"],
        stats["frames_head_eye_ok"],
        stats["blinks_count"],
    )

    if frames_total == 0:
        if raw_frames_total == 0:
            reason = "no_video_frames"
            detail = "ë…¹í™”ëœ ì˜ìƒ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”."
            logger.error(
                "[EXPR] NO_VIDEO_FRAMES raw_frames_total=0 â†’ ì˜ìƒì´ ë¹„ì–´ ìˆìŒ or ì½ê¸° ì‹¤íŒ¨"
            )
        else:
            reason = "no_face_detected"
            detail = "ì˜ìƒì—ì„œ ì–¼êµ´ì„ ì¸ì‹í•˜ì§€ ëª»í•´ í‘œì • ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(
                "[EXPR] NO_FACE_DETECTED raw_frames_total=%s frames_with_face=%s â†’ í•œ ë²ˆë„ ì–¼êµ´ì„ ëª» ì°¾ìŒ",
                raw_frames_total,
                frames_with_face,
            )
        return {
            "expression_analysis": None,
            "aux": {
                "head_gaze_rate_percent": 0.0,
                "eye_only_gaze_rate_percent": 0.0,
                "blinks_count": int(stats["blinks_count"]),
                "blinks_per_min": None,
                "baseline_source": None,
                "frames_used": 0,
                # ë””ë²„ê¹…/í”„ë¡ íŠ¸ìš© ì¶”ê°€ ì •ë³´
                "raw_frames_total": int(raw_frames_total),
                "frames_with_face": int(frames_with_face),
                "reason": reason,
            },
            "overall_score": None,
            "feedback_summary": detail,
            "status": "analysis_unavailable",
            "error_code": reason,
        }

    # ì˜ìƒ ê¸¸ì´(ì´ˆ)
    duration_sec = frames_total / (fps / max(1, frame_stride))
    dur = max(1e-6, duration_sec)

    # ì£¼ì‹œìœ¨
    head_gaze_rate = (stats["frames_head_ok"] / max(1, frames_total)) * 100.0
    head_eye_gaze_rate = (stats["frames_head_eye_ok"] / max(1, frames_total)) * 100.0
    eye_only_gaze_rate = (stats["frames_eye_ok"] / max(1, stats["frames_eye_valid"])) * 100.0

    # ê¹œë¹¡ì„/ë¶„
    blinks_per_min = stats["blinks_count"] / (dur / 60.0)

    # ì‹œì„  ì§€í‘œ(ê³ ì •ë„)
    metrics_head = compute_fixation_metrics(gaze_head_x, gaze_head_y, target=(0.0, 0.0))
    metrics_both = compute_fixation_metrics(gaze_both_x, gaze_both_y, target=(0.0, 0.0))

    # ì ìˆ˜/ë“±ê¸‰/ìš”ì•½ ì‚°ì¶œ
    gaze_rate_norm = np.clip(head_eye_gaze_rate / 100.0, 0.0, 1.0)
    gaze_grade = grade_from_rate(gaze_rate_norm)

    blink_stability = float(max(0.0, 1.0 - min(1.0, blinks_per_min / float(blink_limit_per_min))))
    blink_grade = grade_from_rate(blink_stability)

    mouth_delta = float((ema["mouth"] or baseline["mouth"]) - baseline["mouth"])
    mouth_grade = grade_mouth(mouth_delta)
    mouth_stability = float(np.clip(1.0 - (abs(mouth_delta) / (MOUTH_DELTA * 2.0)), 0.0, 1.0))

    overall_score = float(
        np.clip((gaze_rate_norm * 0.7 + blink_stability * 0.2 + mouth_stability * 0.1) * 100.0, 0.0, 100.0)
    )

    feedback_summary = build_feedback_summary(gaze_grade, blink_grade, mouth_grade)

    logger.info(
        "[EXPR] DONE duration=%.2fs head_eye_gaze_rate=%.1f%% blinks_per_min=%.2f overall_score=%.1f baseline_from_video=%s",
        dur,
        head_eye_gaze_rate,
        blinks_per_min,
        overall_score,
        baseline_from_video,
    )

    result = {
        "expression_analysis": {
            "head_eye_gaze_rate": {"value": round(gaze_rate_norm, 2), "rating": gaze_grade},
            "blink_stability": {"value": round(blink_stability, 2), "rating": blink_grade},
            "mouth_delta": {"value": round(mouth_delta, 3), "rating": mouth_grade},
            "fixation_metrics": {
                "MAE": safe_num(metrics_both.get("MAE")),
                "BCEA": safe_num(metrics_both.get("BCEA")),
            },
        },
        "aux": {
            "head_gaze_rate_percent": round(head_gaze_rate, 1),
            "eye_only_gaze_rate_percent": round(eye_only_gaze_rate, 1),
            "blinks_count": int(stats["blinks_count"]),
            "blinks_per_min": safe_num(blinks_per_min),
            "baseline_source": "ì˜ìƒ ì´ˆë°˜ ê¸°ì¤€" if baseline_from_video else "ì„¸ì…˜ ê¸°ì¤€",
            "frames_used": int(frames_total),
        },
        "overall_score": round(overall_score, 1),
        "feedback_summary": feedback_summary,
    }
    return result



# ì„¸ì…˜ ë‹¨ìœ„ ë¶„ì„ + DB ì €ì¥ + ì‘ë‹µ ìƒì„±

FFMPEG_BIN = which("ffmpeg") or "ffmpeg"
async def run_expression_analysis_for_session(
    session_id: int,
    attempt_id: int,
    blink_limit_per_min: int,
    baseline_seconds: float,
    frame_stride: int,
    db: Session,
):
    # 1) ì´ ì„¸ì…˜ + attempt ì— í•´ë‹¹í•˜ëŠ” ë¹„ë””ì˜¤ media_asset ì°¾ê¸°
    media = (
        db.query(MediaAsset)
        .filter(
            MediaAsset.session_id == session_id,
            MediaAsset.attempt_id == attempt_id,
            MediaAsset.kind == 1,  # 1 = video
        )
        .first()
    )

    if media is None:
        raise HTTPException(status_code=404, detail="session_not_found")

    storage_path = media.storage_url  # ì˜ˆ: "sessions/22/attempt_44.webm"

    # 2) Supabase Storage ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    try:
        file_bytes: bytes = (
            supabase
            .storage
            .from_(BUCKET_NAME)
            .download(storage_path)
        )
    except Exception:
        raise HTTPException(status_code=404, detail="Session with this ID not found")

    if not file_bytes:
        raise HTTPException(status_code=500, detail="expression_empty_video_file")

    # ì›ë³¸ í™•ì¥ì (.webm, .mp4 ë“±)
    ext = Path(storage_path).suffix.lower() or ".webm"

    # 3) ì›ë³¸(webm/...) ì„ì‹œ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp_in:
        tmp_in.write(file_bytes)
        in_path = tmp_in.name

    video_path = in_path
    out_path = None

    # 4) webm ì´ë©´ ffmpeg ë¡œ mp4 ë¡œ ë³€í™˜
    if ext == ".webm":
        fd, out_path = tempfile.mkstemp(suffix=".mp4")
        os.close(fd)

        cmd = [
            FFMPEG_BIN,
            "-y",
            "-i", in_path,
            "-vcodec", "libx264",
            "-an",           # ì˜¤ë””ì˜¤ëŠ” í•„ìš” ì—†ìœ¼ë‹ˆ ì œê±°
            out_path,
        ]
        proc = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        if proc.returncode != 0:
            # ë””ë²„ê¹…í•  ë•Œ stderr ë³´ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì— print í•œ ì¤„ ì¶”ê°€í•´ë„ ë¨
            # print("FFMPEG ERR:", proc.stderr.decode("utf-8", "ignore"))
            raise HTTPException(status_code=500, detail="expression_ffmpeg_failed")

        video_path = out_path  # ì´í›„ ë¶„ì„ì€ mp4 ëŒ€ìƒìœ¼ë¡œ ì§„í–‰

    # 5) í‘œí˜„ ë¶„ì„ ì‹¤í–‰
    try:
        res = analyze_expression_video(
            video_path=video_path,
            blink_limit_per_min=blink_limit_per_min,
            baseline_seconds=baseline_seconds,
            frame_stride=frame_stride,
        )
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for p in {in_path, out_path}:
            if p and os.path.exists(p):
                try:
                    os.remove(p)
                except OSError:
                    pass

        # ğŸ”¹ 5-1) ë¶„ì„ ë¶ˆê°€(status=analysis_unavailable)ì¸ ê²½ìš°: DBì— ì ìˆ˜ ì•ˆ ì“°ê³  ê·¸ëŒ€ë¡œ ë¦¬í„´
        if res.get("status") == "analysis_unavailable" or res.get("expression_analysis") is None:
            logger.info(
                "[EXPR] analysis_unavailable session_id=%s attempt_id=%s reason=%s",
                session_id,
                attempt_id,
                res.get("error_code"),
            )
            # DBì— summary ë ˆì½”ë“œ í•˜ë‚˜ ì •ë„ëŠ” ë‚¨ê¸°ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì„œ commentë§Œ ì €ì¥í•´ë„ ë¨ (ì„ íƒ)
            # ì§€ê¸ˆì€ ì¼ë‹¨ DB ê±´ë“œë¦¬ì§€ ì•Šê³  ë°”ë¡œ ì‘ë‹µë§Œ ë‚´ë ¤ì¤Œ
            return {
                "message": "expression_analysis_unavailable",
                "session_id": session_id,
                "attempt_id": attempt_id,
                **res,
            }

    # 6) feedback_summary í…Œì´ë¸” ì €ì¥/ì—…ë°ì´íŠ¸
    summary = (
        db.query(FeedbackSummary)
        .filter(
            FeedbackSummary.session_id == session_id,
            FeedbackSummary.attempt_id == attempt_id,
        )
        .first()
    )

    if summary is None:
        summary = FeedbackSummary(
            session_id=session_id,
            attempt_id=attempt_id,
        )

    summary.overall_face = res["overall_score"]
    summary.gaze = res["expression_analysis"]["head_eye_gaze_rate"]["value"]
    summary.eye_blink = res["expression_analysis"]["blink_stability"]["value"]
    summary.mouth = res["expression_analysis"]["mouth_delta"]["value"]
    summary.comment = res["feedback_summary"]

    db.add(summary)
    db.commit()

    return {
        "message": "expression_analysis_success",
        "session_id": session_id,
        "attempt_id": attempt_id,
        "overall_score": res["overall_score"],
        "expression_analysis": res["expression_analysis"],
        "feedback_summary": res["feedback_summary"],
    }